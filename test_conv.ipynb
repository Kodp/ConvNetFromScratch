{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.默认加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import os\n",
    "    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Vgg16'\n",
    "    GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "    print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "    import sys\n",
    "    sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import time, os, torch, torchvision, random, time, math\n",
    "from torch import Tensor\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['font.size'] = 10\n",
    "from toolset.utils import *\n",
    "from toolset.data import *\n",
    "from toolset.helper import *\n",
    "from toolset.solver import *\n",
    "from convolutional_networks import *\n",
    "from fully_connected_networks import *\n",
    "from toolset import *\n",
    "if torch.cuda.is_available:\n",
    "    print('Good to go!')\n",
    "else:\n",
    "    print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. unfold()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor.unfold()`是一个函数，用于创建一个视图，其中沿着指定维度展开的数据被组织成一个新的最后一维。换句话说，这个操作可以用于有效地提取张量的滑动窗口块，用于进一步的操作（如卷积）。\n",
    "\n",
    "这个函数的语法是这样的：\n",
    "\n",
    "`tensor.unfold(dimension, size, step)`\n",
    "\n",
    "- `dimension` 是你想要展开的维度\n",
    "- `size` 是滑动窗口的大小\n",
    "- `step` 是滑动窗口移动的步长\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [4, 5, 6],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 8)  # tensor([1, 2, 3, 4, 5, 6, 7])\n",
    "y = x.unfold(0, 3, 1)  # 连续的滑动窗口\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29]])\n",
      "2 3\n",
      "torch.Size([2, 3, 2, 2])\n",
      "tensor([[[[ 0,  1],\n",
      "          [ 6,  7]],\n",
      "\n",
      "         [[ 2,  3],\n",
      "          [ 8,  9]],\n",
      "\n",
      "         [[ 4,  5],\n",
      "          [10, 11]]],\n",
      "\n",
      "\n",
      "        [[[12, 13],\n",
      "          [18, 19]],\n",
      "\n",
      "         [[14, 15],\n",
      "          [20, 21]],\n",
      "\n",
      "         [[16, 17],\n",
      "          [22, 23]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(30).view(5, 6)\n",
    "print(x)\n",
    "H, W = x.shape\n",
    "HH = WW = 2\n",
    "pad = 0\n",
    "stride = 2\n",
    "H_out = 1 + (H + 2 * pad - HH) // stride\n",
    "W_out = 1 + (W + 2 * pad - WW) // stride\n",
    "\n",
    "print(H_out,W_out)\n",
    "\n",
    "def get_conv_table(x, HH, WW, stride):\n",
    "    #@ 针对按顺序的两维(这里是0，1)逐个unfold就可以得到要卷积的部分\n",
    "    y = x.unfold(0, HH, stride).unfold(1, WW, stride)\n",
    "    return y\n",
    "y = get_conv_table(x, HH,WW,stride)\n",
    "#print(y)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "# 输出：\n",
    "# tensor([[[[1, 2],\n",
    "#            [4, 5]],\n",
    "#           [[2, 3],\n",
    "#            [5, 6]]],\n",
    "#          [[[4, 5],\n",
    "#            [7, 8]],\n",
    "#           [[5, 6],\n",
    "#            [8, 9]]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. einsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 转置\n",
    "x = torch.arange(6).reshape((2,3))\n",
    "print(x)\n",
    "# tensor([[0, 1, 2],\n",
    "#         [3, 4, 5]])\n",
    "\n",
    "# 使用einsum进行转置操作\n",
    "y = torch.einsum('ij->ji', x)\n",
    "print(y)\n",
    "# tensor([[0, 3],\n",
    "#         [1, 4],\n",
    "#         [2, 5]])\n",
    "\n",
    "# 最后两维转置\n",
    "a = torch.randn(2,3,5,7,9)\n",
    "# i = 7, j = 9\n",
    "b = torch.einsum('...ij->...ji', [a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15, 18, 21],\n",
      "        [42, 54, 66]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "x = torch.arange(6).reshape((2,3))\n",
    "y = torch.arange(9).reshape((3,3))\n",
    "\n",
    "# 使用einsum进行矩阵乘法操作\n",
    "z = torch.einsum('ij,jk->ik', x, y)\n",
    "print(z)\n",
    "# tensor([[15, 18, 21],\n",
    "#         [42, 54, 66]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2]) tensor([3, 4, 5])\n",
      "tensor([ 0,  4, 10])\n",
      "tensor(14)\n"
     ]
    }
   ],
   "source": [
    "# 张量点乘求和\n",
    "x = torch.arange(3)\n",
    "y = torch.arange(3, 6)\n",
    "print(x, y)\n",
    "# 使用einsum进行点乘操作\n",
    "z = torch.einsum('i,i->i', x, y)\n",
    "print(z)\n",
    "# tensor([ 0,  4, 10])\n",
    "\n",
    "# 使用einsum进行点乘+求和操作\n",
    "z = torch.einsum('i,i->', x, y)\n",
    "print(z)\n",
    "# tensor(14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3,  4,  5],\n",
      "         [ 6,  7,  8,  9, 10, 11],\n",
      "         [12, 13, 14, 15, 16, 17],\n",
      "         [18, 19, 20, 21, 22, 23]],\n",
      "\n",
      "        [[24, 25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34, 35],\n",
      "         [36, 37, 38, 39, 40, 41],\n",
      "         [42, 43, 44, 45, 46, 47]]])\n",
      "2 3\n",
      "torch.Size([2, 2, 3, 2, 2])\n",
      "tensor([[[[[ 0,  1],\n",
      "           [ 6,  7]],\n",
      "\n",
      "          [[ 2,  3],\n",
      "           [ 8,  9]],\n",
      "\n",
      "          [[ 4,  5],\n",
      "           [10, 11]]],\n",
      "\n",
      "\n",
      "         [[[12, 13],\n",
      "           [18, 19]],\n",
      "\n",
      "          [[14, 15],\n",
      "           [20, 21]],\n",
      "\n",
      "          [[16, 17],\n",
      "           [22, 23]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[24, 25],\n",
      "           [30, 31]],\n",
      "\n",
      "          [[26, 27],\n",
      "           [32, 33]],\n",
      "\n",
      "          [[28, 29],\n",
      "           [34, 35]]],\n",
      "\n",
      "\n",
      "         [[[36, 37],\n",
      "           [42, 43]],\n",
      "\n",
      "          [[38, 39],\n",
      "           [44, 45]],\n",
      "\n",
      "          [[40, 41],\n",
      "           [46, 47]]]]])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(2*24).view(2, 4, 6)  # 理解成双通道图像\n",
    "print(x)\n",
    "C, H, W = x.shape\n",
    "HH = WW = 2\n",
    "pad = 0\n",
    "stride = 2\n",
    "H_out = 1 + (H + 2 * pad - HH) // stride\n",
    "W_out = 1 + (W + 2 * pad - WW) // stride\n",
    "\n",
    "print(H_out,W_out)\n",
    "\n",
    "def get_conv_table(x, HH, WW, stride):\n",
    "    #@ 针对按顺序的两维(这里是1，2)逐个unfold就可以得到要卷积的部分\n",
    "    y = x.unfold(1, HH, stride).unfold(2, WW, stride)\n",
    "    return y\n",
    "y = get_conv_table(x, HH,WW,stride)\n",
    "#print(y)\n",
    "print(y.shape)  # (N, H_out, W_out, stride, stride)\n",
    "print(y)\n",
    "\n",
    "# 对角取元素\n",
    "# 双通道图像需要双通道卷积核(单个)\n",
    "w = torch.tensor([\n",
    "    [[1, 0],\n",
    "     [0, 1]],\n",
    "    [[1, 0],\n",
    "     [0, 1]]\n",
    "    \n",
    "])\n",
    "print(w.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "假设你的输入张量 `x` 的形状为 `(C, H, W)`，即通道数为 `C`，高度为 `H`，宽度为 `W`。你的卷积核 `w` 的形状为 `(C, HH, WW)`，即通道数也为 `C`，高度为 `HH`，宽度为 `WW`。此处的 `y` 是 `x` 通过 `unfold` 操作后的结果，形状为 `(C, H_out, W_out, HH, WW)`，其中 `H_out` 和 `W_out` 分别是输出的高度和宽度。\n",
    "\n",
    "下面的 `einsum` 表达式 `'ChwIJ,CIJ->hw'` 可以表示为以下数学公式：\n",
    "\n",
    "$$out_{hw}=\\sum_{C,\\text{堆叠相加}}{\\sum_{I=0}^{HH-1}{\\sum_{J=0}^{WW-1}{y_{ChwIJ}\\cdot}}}w_{CIJ}$$\n",
    "\n",
    "其中，`h` 和 `w` 是输出张量的高度和宽度的索引，`C` 是通道的索引，`I` 和 `J` 是卷积核的高度和宽度的索引。这个表达式的含义是，对于输出张量的每一个位置 `(h, w)`，我们遍历输入的所有通道和卷积核的所有位置，将输入的对应部分和卷积核的元素相乘，然后将所有结果相加，得到输出张量的该位置的值。\n",
    "\n",
    "这就是使用 `einsum` 进行卷积运算的数学公式表示。你可以看到，这个表达式就是卷积运算的定义：将输入的每一个局部窗口和卷积核进行对应元素的乘积和操作，得到输出的对应位置的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 62,  70,  78],\n",
      "        [110, 118, 126]])\n"
     ]
    }
   ],
   "source": [
    "# h, w是输出维度，I，J是卷积核大小\n",
    "out = torch.einsum('ChwIJ,CIJ->hw', y, w)\n",
    "print(out)\n",
    "# tensor([[ 0+7+24+31,2+9+26+33,  78], \n",
    "#         [110, 118, 126]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "关于 `'NChwIJ,FCIJ->NFhw'` 这个表达式的数学公式如下：\n",
    "$$\n",
    "\\text{out}_{NFhw} = \\sum_{C} \\sum_{I=0}^{HH-1} \\sum_{J=0}^{WW-1} x_{NChwIJ} \\cdot w_{FCIJ}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvMy(object):\n",
    "    @staticmethod\n",
    "    def forward(x:Tensor, w:Tensor, b:Tensor, conv_param: dict):\n",
    "        \"\"\"\n",
    "        卷积层前向传播的高速实现。\n",
    "        对输入张量执行2d卷积的函数。\n",
    "        Args:\n",
    "            x: (Tensor): 输入张量，形状为 (N, C, H, W)，其中N为批大小，C为通道数，H为图像高度，W为图像宽度。\n",
    "            w: (Tensor): 卷积层的权重（即“滤波器”），形状为 (F, C, I, J)，其中F为滤波器数量，I,J代表滤波器的高度和宽度。 \n",
    "                这里不采用HH，WW是为了配合einsum下标。\n",
    "            b: (Tensor): 每个卷积核的偏置项。这是一个长度为F的一维张量(F,)。\n",
    "            conv_param (dict): 一个具有两个键'stride'和'pad'的字典，表示在卷积操作中要使用的步长和填充。\n",
    "        Return:\n",
    "            Tensor: 卷积操作的输出。\n",
    "            Tuple: 计算过程的cache元组，(x, w, b, conv_param)。\n",
    "        \"\"\"\n",
    "        stride, pad = conv_param['stride'], conv_param['pad']\n",
    "        N, C, H, W = x.shape\n",
    "        F, C, I, J = w.shape  # I,J为卷积核大小\n",
    "        # 填充\n",
    "        if pad != 0:\n",
    "            x_padded = torch.zeros(N, C, H + 2 * pad, W + 2 * pad, dtype=x.dtype, device=x.device)\n",
    "            x_padded[:, :, pad:-pad, pad:-pad] = x\n",
    "        else:\n",
    "            x_padded = x\n",
    "            \n",
    "        # 不需要手动计算H_out，W_out，下面einsum中用h，w代替\n",
    "        x_padded = x_padded.unfold(2, I, stride).unfold(3, J, stride)  # 展开成为卷积的形式\n",
    "        # 'NChwIJ,FCIJ->NFhw'等式表示我们沿着高度和宽度维度（IJ）将输入窗口和滤波器进行点积，并沿输入通道（C）求和。\n",
    "        out = torch.einsum('NChwIJ,FCIJ->NFhw', x_padded, w)  # (N,F,H_out,W_out)\n",
    "        out += b.reshape(1, -1, 1, 1)\n",
    "        return out, (x_padded, w, b, conv_param)\n",
    "    \n",
    "    \n",
    "    @staticmethod \n",
    "    def backward(dout:Tensor, cache:Tuple):\n",
    "        \"\"\"\n",
    "        卷积层反向传播的高速实现。\n",
    "        输入：\n",
    "        - dout：上游导数。 形状为 (N, F, H_out, W_out) 或者 (N, F, h, w)\n",
    "        - cache：与conv_forward_naive中的(x, w, b, conv_param)相同的元组。\n",
    "\n",
    "        返回一个元组：\n",
    "        - dx：相对于x的梯度\n",
    "        - dw：相对于w的梯度\n",
    "        - db：相对于b的梯度\n",
    "        \"\"\"\n",
    "        # @  修正该函数，也许你需要dive into einsum\n",
    "        # @ https://zhuanlan.zhihu.com/p/361209187\n",
    "        x_padded, w, b, conv_param = cache\n",
    "        stride, pad = conv_param['stride'], conv_param['pad']\n",
    "        db = dout.sum(dim=(0,2,3))  # (F,)\n",
    "        dw = torch.einsum('NChwIJ,FCIJ->FCIJ', x_padded, dout)  # (F,C,I,J)\n",
    "        w_flip = w.flip([2, 3])  # 沿着I和J轴进行翻转\n",
    "        dx_padded = torch.einsum('Nfhw,FCIJ->NChwIJ', dout, w_flip)\n",
    "        \n",
    "        # 如果进行了padding，需要将dx进行切割，去掉padding部分\n",
    "        if pad != 0:\n",
    "            dx = dx_padded[:, :, pad:-pad, pad:-pad]\n",
    "        else:\n",
    "            dx = dx_padded\n",
    "        return dx, dw, db\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): operands do not broadcast with remapped shapes [original->remapped]: [10, 3, 16, 16, 3, 3]->[1, 3, 3, 3, 10, 16, 16] [10, 25, 16, 16]->[10, 25, 16, 16, 1, 1, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m dx_fast_cuda, dw_fast_cuda, db_fast_cuda \u001b[39m=\u001b[39m FastConv\u001b[39m.\u001b[39mbackward(dout_cuda, cache_fast_cuda)\n\u001b[0;32m     41\u001b[0m t3 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m---> 42\u001b[0m dx_my, dw_my, db_my \u001b[39m=\u001b[39m ConvMy\u001b[39m.\u001b[39;49mbackward(dout_cuda, cache_my)\n\u001b[0;32m     43\u001b[0m t4 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m     44\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTesting FastConv.backward:\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[115], line 51\u001b[0m, in \u001b[0;36mConvMy.backward\u001b[1;34m(dout, cache)\u001b[0m\n\u001b[0;32m     49\u001b[0m stride, pad \u001b[39m=\u001b[39m conv_param[\u001b[39m'\u001b[39m\u001b[39mstride\u001b[39m\u001b[39m'\u001b[39m], conv_param[\u001b[39m'\u001b[39m\u001b[39mpad\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     50\u001b[0m db \u001b[39m=\u001b[39m dout\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m))  \u001b[39m# (F,)\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m dw \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49meinsum(\u001b[39m'\u001b[39;49m\u001b[39mNChwIJ,FCIJ->FCIJ\u001b[39;49m\u001b[39m'\u001b[39;49m, x_padded, dout)  \u001b[39m# (F,C,I,J)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m w_flip \u001b[39m=\u001b[39m w\u001b[39m.\u001b[39mflip([\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m])  \u001b[39m# 沿着I和J轴进行翻转\u001b[39;00m\n\u001b[0;32m     53\u001b[0m dx_padded \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mNfhw,FCIJ->NChwIJ\u001b[39m\u001b[39m'\u001b[39m, dout, w_flip)\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\torch3.9\\lib\\site-packages\\torch\\functional.py:360\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[39m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[39m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m--> 360\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: einsum(): operands do not broadcast with remapped shapes [original->remapped]: [10, 3, 16, 16, 3, 3]->[1, 3, 3, 3, 10, 16, 16] [10, 25, 16, 16]->[10, 25, 16, 16, 1, 1, 1]"
     ]
    }
   ],
   "source": [
    "# Rel errors should be around e-11 or less\n",
    "# time.time精度不够会造成除0问题，把所有的time.time换成了time.perf_counter()\n",
    "from convolutional_networks import Conv, FastConv\n",
    "\n",
    "reset_seed(0)\n",
    "x = torch.randn(10, 3, 31, 31, dtype=torch.float64, device='cuda')\n",
    "w = torch.randn(25, 3, 3, 3, dtype=torch.float64, device='cuda')\n",
    "b = torch.randn(25, dtype=torch.float64, device='cuda')\n",
    "dout = torch.randn(10, 25, 16, 16, dtype=torch.float64, device='cuda')\n",
    "x_cuda, w_cuda, b_cuda, dout_cuda = x.to('cuda'), w.to('cuda'), b.to('cuda'), dout.to('cuda')\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "out_naive, cache_naive = Conv.forward(x, w, b, conv_param)\n",
    "# t1 = time.perf_counter()\n",
    "# out_fast, cache_fast = FastConv.forward(x, w, b, conv_param)\n",
    "# t2 = time.perf_counter()\n",
    "# out_fast_cuda, cache_fast_cuda = FastConv.forward(x_cuda, w_cuda, b_cuda, conv_param)\n",
    "# t3 = time.perf_counter()\n",
    "# out_my, cache_my = ConvMy.forward(x, w, b, conv_param)\n",
    "# t4 = time.perf_counter()\n",
    "\n",
    "# print('Testing FastConv.forward:')\n",
    "# print('Naive: %fs' % (t1 - t0))\n",
    "# print('Fast: %fs' % (t2 - t1))\n",
    "# print('Fast CUDA: %fs' % (t3 - t2))\n",
    "# print(f'My Fast:{t4-t3:.6f}s')\n",
    "# print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "# print('Speedup CUDA: %fx' % ((t1 - t0) / (t3 - t2)))\n",
    "# print('Difference: ', grad.rel_error(out_naive, out_fast))\n",
    "# print('Difference CUDA: ', grad.rel_error(out_naive, out_fast_cuda.to(out_naive.device)))\n",
    "# print('Difference My Fast: ', grad.rel_error(out_naive, out_my))\n",
    "\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "dx_naive, dw_naive, db_naive = Conv.backward(dout, cache_naive)\n",
    "t1 = time.perf_counter()\n",
    "dx_fast, dw_fast, db_fast = FastConv.backward(dout, cache_fast)\n",
    "t2 = time.perf_counter()\n",
    "dx_fast_cuda, dw_fast_cuda, db_fast_cuda = FastConv.backward(dout_cuda, cache_fast_cuda)\n",
    "t3 = time.perf_counter()\n",
    "dx_my, dw_my, db_my = ConvMy.backward(dout_cuda, cache_my)\n",
    "t4 = time.perf_counter()\n",
    "print('\\nTesting FastConv.backward:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Fast CUDA: %fs' % (t3 - t2))\n",
    "print(f'My Fast:{t4-t3:.6f}s')\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('Speedup CUDA: %fx' % ((t1 - t0) / (t3 - t2)))\n",
    "print('dx difference: ', grad.rel_error(dx_naive, dx_fast))\n",
    "print('dw difference: ', grad.rel_error(dw_naive, dw_fast))\n",
    "print('db difference: ', grad.rel_error(db_naive, db_fast))\n",
    "print('dx difference CUDA: ', grad.rel_error(dx_naive, dx_fast_cuda.to(dx_naive.device)))\n",
    "print('dw difference CUDA: ', grad.rel_error(dw_naive, dw_fast_cuda.to(dw_naive.device)))\n",
    "print('db difference CUDA: ', grad.rel_error(db_naive, db_fast_cuda.to(db_naive.device)))\n",
    "\n",
    "print('dx difference My: ', grad.rel_error(dx_naive, dx_my.to(dx_naive.device)))\n",
    "print('dw difference My: ', grad.rel_error(dw_naive, dw_my.to(dw_naive.device)))\n",
    "print('db difference My: ', grad.rel_error(db_naive, db_my.to(db_naive.device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
