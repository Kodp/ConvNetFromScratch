{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.默认加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import os\n",
    "    GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Vgg16'\n",
    "    GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "    print(os.listdir(GOOGLE_DRIVE_PATH))\n",
    "    import sys\n",
    "    sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "import time, os, torch, torchvision, random, time, math\n",
    "from torch import Tensor\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['font.size'] = 10\n",
    "from toolset.utils import *\n",
    "from toolset.data import *\n",
    "from toolset.helper import *\n",
    "from toolset.solver import *\n",
    "from convolutional_networks import *\n",
    "from fully_connected_networks import *\n",
    "from toolset import *\n",
    "from typing import Dict, List, Optional\n",
    "TensorDict = Dict[str, torch.Tensor]\n",
    "if torch.cuda.is_available:\n",
    "    print('Good to go!')\n",
    "else:\n",
    "    print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. unfold()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.Tensor.unfold()`是一个函数，用于创建一个视图，其中沿着指定维度展开的数据被组织成一个新的最后一维。换句话说，这个操作可以用于有效地提取张量的滑动窗口块，用于进一步的操作（如卷积）。\n",
    "\n",
    "这个函数的语法是这样的：\n",
    "\n",
    "`tensor.unfold(dimension, size, step)`\n",
    "\n",
    "- `dimension` 是你想要展开的维度\n",
    "- `size` 是滑动窗口的大小\n",
    "- `step` 是滑动窗口移动的步长\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [4, 5, 6],\n",
      "        [5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1, 8)  # tensor([1, 2, 3, 4, 5, 6, 7])\n",
    "y = x.unfold(0, 3, 1)  # 连续的滑动窗口\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29]])\n",
      "2 3\n",
      "torch.Size([2, 3, 2, 2])\n",
      "tensor([[[[ 0,  1],\n",
      "          [ 6,  7]],\n",
      "\n",
      "         [[ 2,  3],\n",
      "          [ 8,  9]],\n",
      "\n",
      "         [[ 4,  5],\n",
      "          [10, 11]]],\n",
      "\n",
      "\n",
      "        [[[12, 13],\n",
      "          [18, 19]],\n",
      "\n",
      "         [[14, 15],\n",
      "          [20, 21]],\n",
      "\n",
      "         [[16, 17],\n",
      "          [22, 23]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(30).view(5, 6)\n",
    "print(x)\n",
    "H, W = x.shape\n",
    "HH = WW = 2\n",
    "pad = 0\n",
    "stride = 2\n",
    "H_out = 1 + (H + 2 * pad - HH) // stride\n",
    "W_out = 1 + (W + 2 * pad - WW) // stride\n",
    "\n",
    "print(H_out,W_out)\n",
    "\n",
    "def get_conv_table(x, HH, WW, stride):\n",
    "    #@ 针对按顺序的两维(这里是0，1)逐个unfold就可以得到要卷积的部分\n",
    "    y = x.unfold(0, HH, stride).unfold(1, WW, stride)\n",
    "    return y\n",
    "y = get_conv_table(x, HH,WW,stride)\n",
    "#print(y)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "# 输出：\n",
    "# tensor([[[[1, 2],\n",
    "#            [4, 5]],\n",
    "#           [[2, 3],\n",
    "#            [5, 6]]],\n",
    "#          [[[4, 5],\n",
    "#            [7, 8]],\n",
    "#           [[5, 6],\n",
    "#            [8, 9]]]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. einsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 转置\n",
    "x = torch.arange(6).reshape((2,3))\n",
    "print(x)\n",
    "# tensor([[0, 1, 2],\n",
    "#         [3, 4, 5]])\n",
    "\n",
    "# 使用einsum进行转置操作\n",
    "y = torch.einsum('ij->ji', x)\n",
    "print(y)\n",
    "# tensor([[0, 3],\n",
    "#         [1, 4],\n",
    "#         [2, 5]])\n",
    "\n",
    "# 最后两维转置\n",
    "a = torch.randn(2,3,5,7,9)\n",
    "# i = 7, j = 9\n",
    "b = torch.einsum('...ij->...ji', [a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[15, 18, 21],\n",
      "        [42, 54, 66]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵乘法\n",
    "x = torch.arange(6).reshape((2,3))\n",
    "y = torch.arange(9).reshape((3,3))\n",
    "\n",
    "# 使用einsum进行矩阵乘法操作\n",
    "z = torch.einsum('ij,jk->ik', x, y)\n",
    "print(z)\n",
    "# tensor([[15, 18, 21],\n",
    "#         [42, 54, 66]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2]) tensor([3, 4, 5])\n",
      "tensor([ 0,  4, 10])\n",
      "tensor(14)\n"
     ]
    }
   ],
   "source": [
    "# 张量点乘求和\n",
    "x = torch.arange(3)\n",
    "y = torch.arange(3, 6)\n",
    "print(x, y)\n",
    "# 使用einsum进行点乘操作\n",
    "z = torch.einsum('i,i->i', x, y)\n",
    "print(z)\n",
    "# tensor([ 0,  4, 10])\n",
    "\n",
    "# 使用einsum进行点乘+求和操作\n",
    "z = torch.einsum('i,i->', x, y)\n",
    "print(z)\n",
    "# tensor(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(N, C, H, W)\n\u001b[0;32m      4\u001b[0m \u001b[39m# print(x)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m out \u001b[39m=\u001b[39m ConvMy\u001b[39m.\u001b[39;49mcol2im(ConvMy\u001b[39m.\u001b[39;49mim2col(x, I, J), x\u001b[39m.\u001b[39;49mshape,I, J)\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(out\u001b[39m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[32], line 99\u001b[0m, in \u001b[0;36mConvMy.col2im\u001b[1;34m(columns, x_shape, I, J, stride)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(I):\n\u001b[0;32m     98\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(J):\n\u001b[1;32m---> 99\u001b[0m         x[:, :, i:H:stride, j:W:stride] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m columns_permuted[:, :, :, :, i, j]\n\u001b[0;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "N, C, h, w, I, J = 2, 3, 2, 4, 3, 3\n",
    "C, H, W = 3, 4, 6\n",
    "x = torch.randn(N, C, H, W)\n",
    "# print(x)\n",
    "out = ConvMy.col2im(ConvMy.im2col(x, I, J), x.shape,I, J)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3,  4,  5],\n",
      "         [ 6,  7,  8,  9, 10, 11],\n",
      "         [12, 13, 14, 15, 16, 17],\n",
      "         [18, 19, 20, 21, 22, 23]],\n",
      "\n",
      "        [[24, 25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34, 35],\n",
      "         [36, 37, 38, 39, 40, 41],\n",
      "         [42, 43, 44, 45, 46, 47]]])\n",
      "2 3\n",
      "torch.Size([2, 2, 3, 2, 2])\n",
      "tensor([[[[[ 0,  1],\n",
      "           [ 6,  7]],\n",
      "\n",
      "          [[ 2,  3],\n",
      "           [ 8,  9]],\n",
      "\n",
      "          [[ 4,  5],\n",
      "           [10, 11]]],\n",
      "\n",
      "\n",
      "         [[[12, 13],\n",
      "           [18, 19]],\n",
      "\n",
      "          [[14, 15],\n",
      "           [20, 21]],\n",
      "\n",
      "          [[16, 17],\n",
      "           [22, 23]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[24, 25],\n",
      "           [30, 31]],\n",
      "\n",
      "          [[26, 27],\n",
      "           [32, 33]],\n",
      "\n",
      "          [[28, 29],\n",
      "           [34, 35]]],\n",
      "\n",
      "\n",
      "         [[[36, 37],\n",
      "           [42, 43]],\n",
      "\n",
      "          [[38, 39],\n",
      "           [44, 45]],\n",
      "\n",
      "          [[40, 41],\n",
      "           [46, 47]]]]])\n",
      "torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(2*24).view(2, 4, 6)  # 理解成双通道图像\n",
    "print(x)\n",
    "C, H, W = x.shape\n",
    "HH = WW = 2\n",
    "pad = 0\n",
    "stride = 2\n",
    "H_out = 1 + (H + 2 * pad - HH) // stride\n",
    "W_out = 1 + (W + 2 * pad - WW) // stride\n",
    "\n",
    "print(H_out,W_out)\n",
    "\n",
    "def get_conv_table(x, HH, WW, stride):\n",
    "    #@ 针对按顺序的两维(这里是1，2)逐个unfold就可以得到要卷积的部分\n",
    "    y = x.unfold(1, HH, stride).unfold(2, WW, stride)\n",
    "    return y\n",
    "y = get_conv_table(x, HH,WW,stride)\n",
    "#print(y)\n",
    "print(y.shape)  # (N, H_out, W_out, stride, stride)\n",
    "print(y)\n",
    "\n",
    "# 对角取元素\n",
    "# 双通道图像需要双通道卷积核(单个)\n",
    "w = torch.tensor([\n",
    "    [[1, 0],\n",
    "     [0, 1]],\n",
    "    [[1, 0],\n",
    "     [0, 1]]\n",
    "    \n",
    "])\n",
    "print(w.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "假设你的输入张量 `x` 的形状为 `(C, H, W)`，即通道数为 `C`，高度为 `H`，宽度为 `W`。你的卷积核 `w` 的形状为 `(C, HH, WW)`，即通道数也为 `C`，高度为 `HH`，宽度为 `WW`。此处的 `y` 是 `x` 通过 `unfold` 操作后的结果，形状为 `(C, H_out, W_out, HH, WW)`，其中 `H_out` 和 `W_out` 分别是输出的高度和宽度。\n",
    "\n",
    "下面的 `einsum` 表达式 `'ChwIJ,CIJ->hw'` 可以表示为以下数学公式：\n",
    "\n",
    "$$out_{hw}=\\sum_{C,\\text{堆叠相加}}{\\sum_{I=0}^{HH-1}{\\sum_{J=0}^{WW-1}{y_{ChwIJ}\\cdot}}}w_{CIJ}$$\n",
    "\n",
    "其中，`h` 和 `w` 是输出张量的高度和宽度的索引，`C` 是通道的索引，`I` 和 `J` 是卷积核的高度和宽度的索引。这个表达式的含义是，对于输出张量的每一个位置 `(h, w)`，我们遍历输入的所有通道和卷积核的所有位置，将输入的对应部分和卷积核的元素相乘，然后将所有结果相加，得到输出张量的该位置的值。\n",
    "\n",
    "这就是使用 `einsum` 进行卷积运算的数学公式表示。你可以看到，这个表达式就是卷积运算的定义：将输入的每一个局部窗口和卷积核进行对应元素的乘积和操作，得到输出的对应位置的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# h, w是输出维度，I，J是卷积核大小\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChwIJ,CIJ->hw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43my\u001b[49m, w)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "# h, w是输出维度，I，J是卷积核大小\n",
    "out = torch.einsum('ChwIJ,CIJ->hw', y, w)\n",
    "print(out)\n",
    "# tensor([[ 0+7+24+31,2+9+26+33,  78], \n",
    "#         [110, 118, 126]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "关于 `'NChwIJ,FCIJ->NFhw'` 这个表达式的数学公式如下：\n",
    "$$\n",
    "\\text{out}_{NFhw} = \\sum_{C} \\sum_{I=0}^{HH-1} \\sum_{J=0}^{WW-1} x_{NChwIJ} \\cdot w_{FCIJ}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvMy(object):\n",
    "    @staticmethod\n",
    "    def forward(x:Tensor, w:Tensor, b:Tensor, conv_param: dict) -> Tuple[Tensor, Tuple[Tensor,Tensor,Tensor,Tensor,Dict]]: \n",
    "        \"\"\"\n",
    "        卷积层前向传播的高速实现。\n",
    "        对输入张量执行2d卷积的函数。\n",
    "        Args:\n",
    "            x: (Tensor): 输入张量，形状为 (N, C, H, W)，其中N为批大小，C为通道数，H为图像高度，W为图像宽度。\n",
    "            w: (Tensor): 卷积层的权重（即“滤波器”），形状为 (F, C, I, J)，其中F为滤波器数量，I,J代表滤波器的高度和宽度。 \n",
    "                这里不采用HH，WW是为了配合einsum下标。\n",
    "            b: (Tensor): 每个卷积核的偏置项。这是一个长度为F的一维张量(F,)。\n",
    "            conv_param (dict): 一个具有两个键'stride'和'pad'的字典，表示在卷积操作中要使用的步长和填充。\n",
    "        Return:\n",
    "            Tensor: 卷积操作的输出。\n",
    "            Tuple: 计算过程的cache元组，(x, w, b, conv_param)。\n",
    "        \"\"\"\n",
    "        stride, pad = conv_param['stride'], conv_param['pad']\n",
    "        N, C, H, W = x.shape\n",
    "        F, C, I, J = w.shape  # I,J为卷积核大小\n",
    "        # 填充\n",
    "        if pad != 0:\n",
    "            x_padded = torch.zeros(N, C, H + 2 * pad, W + 2 * pad, dtype=x.dtype, device=x.device)\n",
    "            x_padded[:, :, pad:-pad, pad:-pad] = x\n",
    "        else:\n",
    "            x_padded = x\n",
    "        # 不需要手动计算H_out，W_out，下面einsum中用h，w代替\n",
    "        x_conv = x_padded.unfold(2, I, stride).unfold(3, J, stride)  #@ NChwIJ 展开成为卷积的形式 \n",
    "        # 'NChwIJ,FCIJ->NFhw'等式表示我们沿着高度和宽度维度（IJ）将输入窗口和滤波器进行点积，并沿输入通道（C）求和。\n",
    "        out = torch.einsum('NChwIJ,FCIJ->NFhw', x_conv, w)  # (N,F,H_out,W_out)\n",
    "        \n",
    "        out += b.reshape(1, -1, 1, 1)\n",
    "        return out, (x.shape, x_conv, w, b, conv_param)\n",
    "    \n",
    "    @staticmethod \n",
    "    def backward(dout:Tensor, cache:Tuple[Tensor,Tensor,Tensor,Tensor,Dict]):\n",
    "        \"\"\"\n",
    "        卷积层反向传播的高速实现。\n",
    "        输入：\n",
    "        - dout：上游导数。 形状为 (N, F, h, w) 或者 (N, F, H_out, W_out) \n",
    "        - cache：与conv_forward_naive中的(x, w, b, conv_param)相同的元组。\n",
    "\n",
    "        返回一个元组：\n",
    "        - dx：相对于x的梯度\n",
    "        - dw：相对于w的梯度\n",
    "        - db：相对于b的梯度\n",
    "        \"\"\"\n",
    "        \n",
    "        x_shape, x_conv,  W, _, conv_param = cache  # x是原始x(NCHW)\n",
    "        F, C, I, J = W.shape\n",
    "        stride, pad = conv_param['stride'], conv_param['pad']\n",
    "        # 计算db\n",
    "        db = dout.sum(dim=(0,2,3))  # (F,)\n",
    "        # 计算dw\n",
    "        dw = torch.einsum('NFhw,NChwIJ->FCIJ', dout, x_conv)  # 不能将NFhw,NChwIJ调换顺序\n",
    "        # 计算dx\n",
    "        w_mat = W.reshape(F, C*I*J).T  # (F,C,I,J)->(F,CIJ)->(CIJ,F)=(A,F)\n",
    "        NAhw = torch.einsum('AF,NFhw->NAhw', w_mat, dout) \n",
    "        # 这里和前面的惯用顺序不同，y=Wx，所以 dL/dx=w.T*dout\n",
    "        dx = ConvMy.NAhw2NCHW(NAhw, x_shape, I, J, stride, pad)  \n",
    "        if pad != 0:\n",
    "            dx = dx[:, :, pad:-pad, pad:-pad]\n",
    "        # dx = torch.nn.grad.conv2d_input(x.shape, w, dout, stride=stride, padding=pad)  # 正确的代码，但使用nn\n",
    "        return dx, dw, db  \n",
    "    \n",
    "    @staticmethod\n",
    "    def NAhw2NCHW(NAhw:Tensor, x_origin_shape, I, J, stride=1, pad=0) -> Tensor:\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            NAhw (torch.Tensor): NAhw格式张量，形状 (N, CIJ, h_out, w_out)\n",
    "            x_shape (tuple): 原始输入张量的形状，(N, C, H, W)\n",
    "            I, J (int): 卷积核的高度和宽度\n",
    "            stride (int): 卷积核的步长，默认为1\n",
    "        返回:\n",
    "            torch.Tensor: 张量，形状 (N, C, H, W)\n",
    "        \"\"\"\n",
    "        N, C, H, W = x_origin_shape  # 原始x_shape\n",
    "        h_out = 1 + (H + 2 * pad - I) // stride \n",
    "        w_out = 1 + (W + 2 * pad - J) // stride \n",
    "        \n",
    "        # 创建一个形状和x_padded张量相同的全零张量\n",
    "        out = torch.zeros(N, C, H + 2 * pad, W + 2 * pad, dtype=NAhw.dtype,device=NAhw.device)\n",
    "        NChwIJ = NAhw.reshape(N, C, I, J, h_out, w_out).permute(0, 1, 4, 5, 2, 3)  # NChwIJ\n",
    "        # 遍历卷积核的每一个元素，恢复原始输入张量\n",
    "        for top in range(I):\n",
    "            down = top + h_out * stride\n",
    "            for left in range(J):\n",
    "                right = left + w_out * stride\n",
    "                out[:, :, top:down:stride, left:right:stride] += NChwIJ[:, :, :, :, top, left]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N, C, h, w, I, J = 2, 3, 2, 4, 3, 3\n",
    "# C, H, W = 3, 4, 6\n",
    "# x = torch.ones(N, C, H, W)\n",
    "# # print(x)\n",
    "# col = ConvMy.im2col(x, I, J)  \n",
    "# print(col.shape) # (N, h*w, C*I*J)\n",
    "# out = ConvMy.col2im(col, x.shape,I, J)\n",
    "# print(out.shape)\n",
    "# print(x, out, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# N, C, h, w, I, J = 2, 3, 2, 4, 3, 3\n",
    "# C, H, W = 3, 4, 6\n",
    "# F = 2\n",
    "# x1 = torch.zeros(N, h, w, C, I, J)\n",
    "# # print(x.shape)\n",
    "# weight = torch.randint(0,10,[F, C, I, J])\n",
    "# w_expand1 = weight[:,None,None,:,:,:]  # (F11CIJ)\n",
    "# print(w_expand1.shape)\n",
    "# for i in range(w_expand1.shape[0]):\n",
    "#     x1 += w_expand1[i] #  (N, h, w, C, I, J) + (1, 1, 1, C, I, J)\n",
    "# # print(x)\n",
    "\n",
    "# x2 = torch.zeros(N, C, h, w, I, J)\n",
    "# w_expand2 = weight[:, :, None, None, :, :] # (FC11IJ)\n",
    "# print(w_expand2.shape)\n",
    "\n",
    "# for i in range(w_expand2.shape[0]):\n",
    "#     x2 += w_expand2[i] #  (N, C, h, w, I, J) + (1, C, 1, 1, I, J)\n",
    "# x1 = x1.permute(0,3,1,2,4,5)\n",
    "# print(x1 == x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing FastConv.forward:\n",
      "Naive: 5.257141s\n",
      "Fast: 0.002498s\n",
      "Fast CUDA: 0.000533s\n",
      "My Fast:0.000838s\n",
      "Speedup: 2104.624364x\n",
      "Speedup CUDA: 9868.859022x\n",
      "Difference:  2.1928544370986248e-16\n",
      "Difference CUDA:  2.1928544370986248e-16\n",
      "Difference My Fast:  2.1928544370986248e-16\n",
      "torch.Size([10, 3, 16, 16]) torch.Size([10, 3, 16, 16])\n",
      "torch.Size([10, 3, 16, 16]) torch.Size([10, 3, 16, 16])\n",
      "torch.Size([10, 3, 16, 16]) torch.Size([10, 3, 16, 16])\n",
      "torch.Size([10, 3, 16, 16]) torch.Size([10, 3, 16, 16])\n",
      "torch.Size([10, 3, 16, 16]) torch.Size([10, 3, 16, 16])\n",
      "torch.Size([10, 3, 16, 16]) torch.Size([10, 3, 16, 16])\n",
      "torch.Size([10, 3, 16, 16]) torch.Size([10, 3, 16, 16])\n",
      "torch.Size([10, 3, 16, 16]) torch.Size([10, 3, 16, 16])\n",
      "torch.Size([10, 3, 16, 16]) torch.Size([10, 3, 16, 16])\n",
      "\n",
      "Testing FastConv.backward:\n",
      "Naive: 8.685450s\n",
      "Fast: 0.001453s\n",
      "Fast CUDA: 0.000421s\n",
      "My Fast:0.001747s\n",
      "Speedup: 5978.421190x\n",
      "Speedup CUDA: 20655.054198x\n",
      "dx difference:  3.4031094481068633e-16\n",
      "dw difference:  2.439673942692602e-15\n",
      "db difference:  2.1450568557297442e-16\n",
      "dx difference CUDA:  3.4031094481068633e-16\n",
      "dw difference CUDA:  2.439673942692602e-15\n",
      "db difference CUDA:  2.1450568557297442e-16\n",
      "dx difference My:  3.4031094481068633e-16\n",
      "dw difference My:  2.6273411690535712e-15\n",
      "db difference My:  2.1450568557297442e-16\n"
     ]
    }
   ],
   "source": [
    "# Rel errors should be around e-11 or less\n",
    "# time.time精度不够会造成除0问题，把所有的time.time换成了time.perf_counter()\n",
    "from convolutional_networks import Conv, FastConv\n",
    "\n",
    "reset_seed(0)\n",
    "x = torch.randn(10, 3, 31, 31, dtype=torch.float64, device='cuda')\n",
    "w = torch.randn(25, 3, 3, 3, dtype=torch.float64, device='cuda')\n",
    "b = torch.randn(25, dtype=torch.float64, device='cuda')\n",
    "dout = torch.randn(10, 25, 16, 16, dtype=torch.float64, device='cuda')\n",
    "x_cuda, w_cuda, b_cuda, dout_cuda = x.to('cuda'), w.to('cuda'), b.to('cuda'), dout.to('cuda')\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "out_naive, cache_naive = Conv.forward(x, w, b, conv_param)\n",
    "t1 = time.perf_counter()\n",
    "out_fast, cache_fast = FastConv.forward(x, w, b, conv_param)\n",
    "t2 = time.perf_counter()\n",
    "out_fast_cuda, cache_fast_cuda = FastConv.forward(x_cuda, w_cuda, b_cuda, conv_param)\n",
    "t3 = time.perf_counter()\n",
    "out_my, cache_my = ConvMy.forward(x, w, b, conv_param)\n",
    "t4 = time.perf_counter()\n",
    "\n",
    "print('Testing FastConv.forward:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Fast CUDA: %fs' % (t3 - t2))\n",
    "print(f'My Fast:{t4-t3:.6f}s')\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('Speedup CUDA: %fx' % ((t1 - t0) / (t3 - t2)))\n",
    "print('Difference: ', grad.rel_error(out_naive, out_fast))\n",
    "print('Difference CUDA: ', grad.rel_error(out_naive, out_fast_cuda.to(out_naive.device)))\n",
    "print('Difference My Fast: ', grad.rel_error(out_naive, out_my))\n",
    "\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "dx_naive, dw_naive, db_naive = Conv.backward(dout, cache_naive)\n",
    "t1 = time.perf_counter()\n",
    "dx_fast, dw_fast, db_fast = FastConv.backward(dout, cache_fast)\n",
    "t2 = time.perf_counter()\n",
    "dx_fast_cuda, dw_fast_cuda, db_fast_cuda = FastConv.backward(dout_cuda, cache_fast_cuda)\n",
    "t3 = time.perf_counter()\n",
    "dx_my, dw_my, db_my = ConvMy.backward(dout_cuda, cache_my)\n",
    "t4 = time.perf_counter()\n",
    "print('\\nTesting FastConv.backward:')\n",
    "print('Naive: %fs' % (t1 - t0))\n",
    "print('Fast: %fs' % (t2 - t1))\n",
    "print('Fast CUDA: %fs' % (t3 - t2))\n",
    "print(f'My Fast:{t4-t3:.6f}s')\n",
    "print('Speedup: %fx' % ((t1 - t0) / (t2 - t1)))\n",
    "print('Speedup CUDA: %fx' % ((t1 - t0) / (t3 - t2)))\n",
    "print('dx difference: ', grad.rel_error(dx_naive, dx_fast))\n",
    "print('dw difference: ', grad.rel_error(dw_naive, dw_fast))\n",
    "print('db difference: ', grad.rel_error(db_naive, db_fast))\n",
    "print('dx difference CUDA: ', grad.rel_error(dx_naive, dx_fast_cuda.to(dx_naive.device)))\n",
    "print('dw difference CUDA: ', grad.rel_error(dw_naive, dw_fast_cuda.to(dw_naive.device)))\n",
    "print('db difference CUDA: ', grad.rel_error(db_naive, db_fast_cuda.to(db_naive.device)))\n",
    "\n",
    "print('dx difference My: ', grad.rel_error(dx_naive, dx_my.to(dx_naive.device)))\n",
    "print('dw difference My: ', grad.rel_error(dw_naive, dw_my.to(dw_naive.device)))\n",
    "print('db difference My: ', grad.rel_error(db_naive, db_my.to(db_naive.device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
